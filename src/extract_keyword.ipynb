{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from soynlp.word import WordExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Sentences:\n",
    "    def __init__(self, fname):\n",
    "        self.fname = fname\n",
    "        self.length = 0\n",
    "    def __iter__(self):\n",
    "        with open(self.fname, encoding='utf-8') as f:\n",
    "            for doc in f:\n",
    "                doc = doc.strip()\n",
    "                if not doc:\n",
    "                    continue\n",
    "                for sent in doc.split('  '):\n",
    "                    yield sent\n",
    "    def __len__(self):\n",
    "        if self.length == 0:\n",
    "            with open(self.fname, encoding='utf-8') as f:\n",
    "                for doc in f:\n",
    "                    doc = doc.strip()\n",
    "                    if not doc:\n",
    "                        continue\n",
    "                    self.length += len(doc.split('  '))\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    with open(filename, 'r' , encoding='utf-8-sig') as f:\n",
    "        data = [line.split('$') for line in f.read().splitlines()]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def word_score(score):\n",
    "    import math\n",
    "    return (score.cohesion_forward * math.exp(score.right_branching_entropy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter\n",
    "twit = Twitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return twit.morphs(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = re.sub('[a-zA-Z]','', text)\n",
    "    cleaned_text = re.sub('[\\{\\}\\[\\]\\/?.,;:|\\)*~`!\\-_+<>@\\#$%&\\\\\\=\\(\\'\\\"ㅋ\\ㅜ\\ㅠ\\ㅎ]', '',\n",
    "                    cleaned_text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"Pi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "review_data = sc.textFile(\"/user/data/data_Review/택시운전사_cleand.txt\").map(lambda line: line.split(\"$\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rating = list([x for x in review_data.map(lambda x:x[:1]).toLocalIterator()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "review = list([clean_text(str(x)).strip() for x in review_data.map(lambda x:x[2:-1]).toLocalIterator()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = [(rating[i][0], review[i]) for i in range(len(rating)) if len(review[i])  >= 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "posData = [(x[1],1) for x in data if int(x[0]) >= 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "negData = [(x[1],0) for x in data if int(x[0]) <= 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = posData + negData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = [x[0] for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = [str(x[1]) for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = sc.textFile(\"/user/data/Test_Train_set/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "header = train.first() #extract header\n",
    "train = train.filter(lambda x: x != header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train = list([x for x in train.map(lambda x:x[:-3]).toLocalIterator()])\n",
    "y_train = list([x for x in train.map(lambda x:x[-1:]).toLocalIterator()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vect_1_5 = TfidfVectorizer(ngram_range=(1,5), min_df = 3, tokenizer=tokenizer).fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nx = vect_1_5.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_grid_5_3 = LogisticRegression(C=10.0, penalty='l2', random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_grid_5_3.fit(nx,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = vect_1_5.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = lr_grid_5_3.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_taxi_re = []\n",
    "neg_taxi_re = []\n",
    "for i in range(len(x)):\n",
    "    if y_pred[i] == '1':\n",
    "        pos_taxi_re.append(x[i])\n",
    "    elif y_pred[i] == '0':\n",
    "        neg_taxi_re.append(x[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pos_taxi_re_temp = pd.DataFrame(pos_taxi_re)\n",
    "pos_taxi_re_temp.to_csv('/home/vagrant/BigdataProject/data/corpus/pos_taxi_re.csv', index=False, header=False, encoding = 'utf-8-sig')  \n",
    "corpus_fname = '/home/vagrant/BigdataProject/data/corpus/pos_taxi_re.csv'\n",
    "pos_sentences = Sentences(corpus_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from soynlp.noun import LRNounExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noun_extractor = LRNounExtractor(min_count=100)\n",
    "noun_extractor.train(pos_sentences)\n",
    "nouns = noun_extractor.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noun_score(score):    \n",
    "    import math\n",
    "    return score.score * score.known_r_ratio * math.log(score.frequency)\n",
    "\n",
    "index = 0\n",
    "posword = []\n",
    "for noun, score in sorted(nouns.items(), key=lambda x:noun_score(x[1]), reverse=True):\n",
    "    print()\n",
    "    if noun == \"진짜\" or noun == \"영화\" or noun == \"정말\" or noun == \"너무\" or noun == \"그냥\" or noun == \"생각\": continue\n",
    "    index += 1\n",
    "    posword.append([noun,score.frequency])\n",
    "    print(noun,score)\n",
    "    if index == 5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#list to RDD\n",
    "data = sc.parallelize(posword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schema_data = data.map(\n",
    "  lambda x: {'word': x[0], 'freq': x[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo_spark\n",
    "pymongo_spark.activate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schema_data.saveToMongoDB('mongodb://localhost:27017/keyword.Dunkirk_Pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#부정키워드 \n",
    "neg_taxi_re_temp = pd.DataFrame(neg_taxi_re)\n",
    "neg_taxi_re_temp.to_csv('/home/vagrant/BigdataProject/data/corpus/neg_taxi_re.csv', index=False, header=False, encoding = 'utf-8-sig')  \n",
    "corpus_fname = '/home/vagrant/BigdataProject/data/corpus/neg_taxi_re.csv'\n",
    "neg_sentences = Sentences(corpus_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_extractor = WordExtractor(min_count= 60,\n",
    "                               min_cohesion_forward=0.5, \n",
    "                               min_right_branching_entropy=0.3)\n",
    "\n",
    "word_extractor.train(neg_sentences)\n",
    "words = word_extractor.extract()\n",
    "\n",
    "\n",
    "negword=[]\n",
    "index = 0;\n",
    "for word, score in sorted(words.items(), key=lambda x:word_score(x[1]), reverse=True)[:20]:\n",
    "    \n",
    "    if word == \"진짜\" or word == \"합니다\" or word == \"영화\" or word == \"정말\" or word == \"너무\" or word == \"그냥\" or word == \"생각\" or word == \"봤는데\" or word ==\"평점\": continue\n",
    "    index += 1\n",
    "    negword.append([word,score.leftside_frequency])\n",
    "    print('%s     (%d, %.3f, %.3f)' % (word, \n",
    "                                   score.leftside_frequency, \n",
    "                                   score.cohesion_forward,\n",
    "                                   score.right_branching_entropy\n",
    "                                  ))\n",
    "    if index == 5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sc.parallelize(negword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schema_data = data.map(\n",
    "  lambda x: {'word': x[0], 'freq': x[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 부정키워드 저장\n",
    "schema_data.saveToMongoDB('mongodb://localhost:27017/keyword.Dunkirk_Neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SUBKEYWORD 추출 함수\n",
    "def ExtractSub(keyword):\n",
    "    review = []\n",
    "    for i in pos_taxi_re:\n",
    "        if keyword in i:\n",
    "            review.append(i) \n",
    "        \n",
    "    역사1 = pd.DataFrame(review)\n",
    "    역사1.to_csv('/home/vagrant/BigdataProject/data/temp/'+keyword+'.csv', index=False, header=False, encoding = 'utf-8-sig')  \n",
    "    corpus_fname = '/home/vagrant/BigdataProject/data/temp/'+keyword+'.csv'\n",
    "    sentences = Sentences(corpus_fname)\n",
    "    \n",
    "    word_extractor = WordExtractor(min_count=1,\n",
    "                                   min_cohesion_forward=0, \n",
    "                                   min_right_branching_entropy=0.0)\n",
    "    \n",
    "    word_extractor.train(sentences)\n",
    "    words = word_extractor.extract()\n",
    "    \n",
    "    \n",
    "    subKeyword=[]\n",
    "    for word, score in sorted(words.items(), key=lambda x:word_score(x[1]), reverse=True)[:20]:\n",
    "        if len(word) > 1 :\n",
    "            if word == \"생각\": continue;\n",
    "            #print('%s     (%d, %.3f, %.3f)' % (word, \n",
    "            #                               score.leftside_frequency, \n",
    "            #                               score.cohesion_forward,\n",
    "            #                               score.right_branching_entropy\n",
    "            #                              ))\n",
    "            subKeyword.append([word,score.leftside_frequency])\n",
    "    \n",
    "    from konlpy.tag import Kkma\n",
    "    kkma = Kkma()\n",
    "    \n",
    "    subKeyword_noun = [(kkma.nouns(row[0]), row[1]) for row in subKeyword]\n",
    "    \n",
    "    \n",
    "    subKeyword_noun2 = []\n",
    "    for i in range(len(subKeyword_noun)):\n",
    "        for j in range(len(subKeyword_noun[i][0])):\n",
    "            if len(subKeyword_noun[i][0][j]) >1:\n",
    "                subKeyword_noun2.append([subKeyword_noun[i][0][j], subKeyword_noun[i][1]])\n",
    "       \n",
    "    subKeyword_noun_set = list(set([row[0] for row in subKeyword_noun2]))\n",
    "    \n",
    "    cnt = 0\n",
    "    subKeyword_noun_res = []\n",
    "    for i in subKeyword_noun_set:\n",
    "        for j in range(len(subKeyword_noun2)):\n",
    "            if i ==  str(subKeyword_noun2[j][0]):\n",
    "                cnt += subKeyword_noun2[j][1]    \n",
    "        subKeyword_noun_res.append([i, cnt, keyword])\n",
    "        cnt= 0\n",
    "                \n",
    "    print(subKeyword_noun_res)\n",
    "    return subKeyword_noun_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 긍정 SubKeyword 저장\n",
    "subkeyword = []\n",
    "for key in range(len(posword)):\n",
    "    subkeyword.append(ExtractSub(posword[key][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,5):\n",
    "    subkeyword[i]\n",
    "    data = sc.parallelize(subkeyword[i])\n",
    "    schema_data = data.map(lambda x: {'word': x[0], 'freq': x[1], 'label': x[2]})\n",
    "    schema_data.saveToMongoDB('mongodb://localhost:27017/keyword.Dunkirk_sub')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 부정 SubKeyword 저장\n",
    "subkeyword = []\n",
    "for key in range(len(negword)):\n",
    "    subkeyword.append(ExtractSub(negword[key][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,5):\n",
    "    subkeyword[i]\n",
    "    data = sc.parallelize(subkeyword[i])\n",
    "    schema_data = data.map(lambda x: {'word': x[0], 'freq': x[1], 'label': x[2]})\n",
    "    schema_data.saveToMongoDB('mongodb://localhost:27017/keyword.Dunkirk_sub')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
